# -*- coding: utf-8 -*-
"""exp1-statical_test.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IWuoYIa2YaASf-osfW9CUOp1WNA0mmEa
"""

from google.colab import drive
drive.mount('/content/drive')

import kagglehub
from kagglehub import KaggleDatasetAdapter

file_name_in_dataset = "CRISPRGeneEffect.csv"

df = kagglehub.dataset_load(
  KaggleDatasetAdapter.PANDAS,
  "alexsychu/depmapcrisprgeneeffect",
  file_name_in_dataset,
)

import matplotlib.pyplot as plt
import pandas as pd
import scipy as sp
import numpy as np

pd.set_option("display.max_columns", None)

genesdf = df

data_path = '/content/drive/MyDrive/MOLSGNN/data/'

import re

def remove_trailing_number(df):
    """
    Remove trailing " (number)" from column names, e.g. "A1BG (1)" -> "A1BG"
    """
    return df.rename(
        columns=lambda c: re.sub(r"\s*\(\d+\)$", "", c) if isinstance(c, str) else c
    )

genesdf = remove_trailing_number(genesdf)
# Instead of dropping, set 'Unnamed: 0' as index if it exists
if 'Unnamed: 0' in genesdf.columns:
    genesdf = genesdf.set_index('Unnamed: 0')
    genesdf.index.name = 'ModelID' # Rename index to match cell_line_mutations for clarity

# Convert all remaining columns (gene effect data) to numeric, coercing errors.
# This ensures that all data used for correlation calculations is numerical.
for col in genesdf.columns:
    genesdf[col] = pd.to_numeric(genesdf[col], errors='coerce')

# quick check
print("Sample columns:", list(genesdf.columns[:20]))
dupes = genesdf.columns[genesdf.columns.duplicated()].unique()
print("Duplicate columns after renaming:", list(dupes))

genesdf.head()

# SL gene pairs df

sldf = pd.read_csv(data_path+'gene_sl_gene.tsv', sep='\t')
sldf.head()

# Use the computational predictions as test set

test_gene_sl = pd.DataFrame(
    sldf[sldf['rel_source'] == "Computational Prediction"]
)

# Non-SL gene pairs df

nsldf = pd.read_csv(data_path+'gene_nonsl_gene.tsv', sep='\t')
nsldf.head()

# cell line mutations df from Depmap

mutations_df = pd.read_csv(data_path+'OmicsSomaticMutations.csv')
# mutations_df = remove_trailing_number(mutations_df)

mutations_df.head()

rows_drop = sldf[sldf['rel_source'] == "Computational Prediction"].index

sldf.drop(rows_drop, inplace=True)
sldf.shape

known_sl_pairs = []

for gene_a, gene_b in zip(sldf.x_name, sldf.y_name):
    # Ensure both gene names are strings and not NaN before adding to the list
    if isinstance(gene_a, str) and isinstance(gene_b, str):
        known_sl_pairs.append((gene_a, gene_b))

print(known_sl_pairs)

non_sl_pairs = []

for gene_a, gene_b in zip(nsldf.x_name, nsldf.y_name):
    # Ensure both gene names are strings and not NaN before adding to the list
    if isinstance(gene_a, str) and isinstance(gene_b, str):
        non_sl_pairs.append((gene_a, gene_b))

print(non_sl_pairs)

def process_detailed_mutations(mutations_df):
        """
        Process detailed mutation data into binary matrix.

        Parameters:
        -----------
        mutations_df : DataFrame
            Detailed mutations with columns: ModelID, HugoSymbol, VariantInfo, etc.

        Returns:
        --------
        mutation_matrix : DataFrame
            Binary matrix (cell_lines x genes)
        """
        print("  Processing detailed mutation data...")

        # Filter for damaging mutations (optional)
        # You can filter by: isCOSMIChotspot, isDeleterious, etc.
        if 'isDeleterious' in mutations_df.columns:
            mutations_df = mutations_df[mutations_df['isDeleterious'] == True]
            print(f"  Filtered to {len(mutations_df)} damaging mutations")

        # Create binary matrix
        # Pivot: rows=ModelID (cell lines), columns=HugoSymbol (genes), values=1 (mutated)
        mutation_matrix = mutations_df.pivot_table(
            index='ModelID',
            columns='HugoSymbol',
            values='VariantInfo',  # Any column works since we just need presence
            aggfunc='count',
            fill_value=0
        )

        # Convert counts to binary (mutated=1, not mutated=0)
        mutation_matrix = (mutation_matrix > 0).astype(int)

        return mutation_matrix

cell_line_mutations = process_detailed_mutations(mutations_df)

cell_line_mutations.shape

mutation_freq_cache = None
mutation_mask_cache = None

def precompute_mutation_stats():
    """
    Precompute mutation statistics for all genes to speed up feature extraction.
    Call this once after loading data, before extracting features for pairs.
    """
    if cell_line_mutations is None:
        print("No mutation data loaded, skipping precomputation.")
        return

    print("Precomputing mutation statistics...")

    # Store mutation frequencies for all genes
    mutation_freq_cache = {}
    for gene in cell_line_mutations.columns:
        mut_values = cell_line_mutations[gene].values
        mutation_freq_cache[gene] = np.mean(mut_values)

    # Precompute masks for faster indexing
    # Store which cell lines have mutations for each gene
    mutation_mask_cache = {}
    for gene in cell_line_mutations.columns:
        mutation_mask_cache[gene] = cell_line_mutations[gene].values > 0

    print(f"Precomputed stats for {len(mutation_freq_cache)} genes")

# Align genesdf and cell_line_mutations by their common ModelIDs
common_model_ids = genesdf.index.intersection(cell_line_mutations.index)

genesdf = genesdf.loc[common_model_ids].copy()
cell_line_mutations = cell_line_mutations.loc[common_model_ids].copy()

precompute_mutation_stats()

def compute_mutation_context_features(gene_a, gene_b, cell_line_mutations):
    """
    Compute context-specific features based on mutations (OPTIMIZED).

    SL is often context-dependent: BRCA1-mutant cells depend on PARP1,
    but BRCA1-wildtype cells don't.

    This is KEY for understanding synthetic lethality mechanisms!

    Parameters:
    -----------
    gene_a, gene_b : str
        Gene symbols

    Returns:
    --------
    features : dict
    """
    features = {}

    if cell_line_mutations is None:
        return {}

    # Quick check if genes exist
    if (gene_a not in genesdf.columns or
        gene_b not in genesdf.columns):
        return {}

    # Use numpy arrays directly (faster than pandas indexing)
    effects_a = genesdf[gene_a].values
    effects_b = genesdf[gene_b].values

    # Precompute valid mask once
    valid_mask = ~(np.isnan(effects_a) | np.isnan(effects_b))
    effects_a_clean = effects_a[valid_mask]
    effects_b_clean = effects_b[valid_mask]

    # Get mutation data
    has_mut_a = gene_a in cell_line_mutations.columns
    has_mut_b = gene_b in cell_line_mutations.columns

    # Use cached masks if available, otherwise compute
    if mutation_mask_cache:
        mut_a_mask = mutation_mask_cache.get(gene_a, None) if has_mut_a else None
        mut_b_mask = mutation_mask_cache.get(gene_b, None) if has_mut_b else None
    else:
        mut_a_mask = (cell_line_mutations[gene_a].values > 0) if has_mut_a else None
        mut_b_mask = (cell_line_mutations[gene_b].values > 0) if has_mut_b else None

    # Apply valid mask to mutation masks
    if mut_a_mask is not None:
        mut_a_mask = mut_a_mask[valid_mask]
    if mut_b_mask is not None:
        mut_b_mask = mut_b_mask[valid_mask]

    # CONTEXT-DEPENDENT DEPENDENCY (vectorized operations)
    if mut_a_mask is not None:
        n_mutated_a = np.sum(mut_a_mask)
        if n_mutated_a > 5:  # Minimum threshold
            # Vectorized mean calculation
            effect_b_when_a_mutated = np.mean(effects_b_clean[mut_a_mask])
            effect_b_when_a_wt = np.mean(effects_b_clean[~mut_a_mask]) if n_mutated_a < len(mut_a_mask) else 0

            features['mutation_context_dependency_a_to_b'] = effect_b_when_a_wt - effect_b_when_a_mutated
            features['mutation_effect_b_in_mutant_a'] = effect_b_when_a_mutated

    if mut_b_mask is not None:
        n_mutated_b = np.sum(mut_b_mask)
        if n_mutated_b > 5:
            effect_a_when_b_mutated = np.mean(effects_a_clean[mut_b_mask])
            effect_a_when_b_wt = np.mean(effects_a_clean[~mut_b_mask]) if n_mutated_b < len(mut_b_mask) else 0

            features['mutation_context_dependency_b_to_a'] = effect_a_when_b_wt - effect_a_when_b_mutated
            features['mutation_effect_a_in_mutant_b'] = effect_a_when_b_mutated

    # MUTATION FREQUENCY (use cache if available)
    if mutation_freq_cache:
        if gene_a in mutation_freq_cache:
            features['mutation_frequency_a'] = mutation_freq_cache[gene_a]
        if gene_b in mutation_freq_cache:
            features['mutation_frequency_b'] = mutation_freq_cache[gene_b]
    else:
        if mut_a_mask is not None:
            features['mutation_frequency_a'] = np.mean(mut_a_mask)
        if mut_b_mask is not None:
            features['mutation_frequency_b'] = np.mean(mut_b_mask)

        # MUTUAL EXCLUSIVITY (vectorized)
        if mut_a_mask is not None and mut_b_mask is not None:
            both_mutated = np.sum(mut_a_mask & mut_b_mask)
            either_mutated = np.sum(mut_a_mask | mut_b_mask)

            features['mutation_co_occurrence_ratio'] = both_mutated / either_mutated if either_mutated > 0 else 0
            features['mutation_both_mutated_count'] = both_mutated
            features['mutation_either_mutated_count'] = either_mutated

        return features

def empty_features():
    """Return zero-filled features when data is missing."""
    return {
        'depmap_pearson_correlation': 0,
        'depmap_spearman_correlation': 0,
        'depmap_conditional_dependency': 0,
        'depmap_mutual_essentiality': 0,
        'depmap_essentiality_diff_std': 0,
        'depmap_essentiality_diff_mean': 0,
        'depmap_mean_effect_a': 0,
        'depmap_mean_effect_b': 0,
        'depmap_std_effect_a': 0,
        'depmap_std_effect_b': 0,
        'depmap_is_essential_a': 0,
        'depmap_is_essential_b': 0,
        'depmap_complementary': 0,
    }

from scipy.stats import pearsonr, spearmanr

def compute_codependency_features(gene_a, gene_b, genesdf):
    """
    Compute co-dependency features between two genes.

    This is the KEY for SL prediction: genes that are synthetic lethal
    show correlated dependency patterns across cell lines.

    Parameters:
    -----------
    gene_a, gene_b : str
        Gene symbols
    genesdf: dataframe
        Gene effect data

    Returns:
    --------
    features : dict
    """
    features = {}

    # Check if genes exist in data
    if gene_a not in genesdf.columns or gene_b not in genesdf.columns:
        return empty_features()

    effects_a = genesdf[gene_a].values
    effects_b = genesdf[gene_b].values

    # Remove NaN values before computing correlations and other statistics
    mask = ~(np.isnan(effects_a) | np.isnan(effects_b))
    effects_a = effects_a[mask]
    effects_b = effects_b[mask]

    if len(effects_a) < 10:  # Not enough data after removing NaNs
        print("Not enough data after removing NaNs")
        return empty_features()

    # 1. PEARSON CORRELATION (linear relationship)
    # Positive correlation = both essential/non-essential together
    # Negative correlation = compensatory relationship
    pearson_corr, _ = pearsonr(effects_a, effects_b)
    features['depmap_pearson_correlation'] = pearson_corr

    # 2. SPEARMAN CORRELATION (monotonic relationship)
    spearman_corr, _ = spearmanr(effects_a, effects_b)
    features['depmap_spearman_correlation'] = spearman_corr

    # 3. CONDITIONAL DEPENDENCY
    # When gene A is essential (negative score), is gene B also essential?
    threshold_a = np.percentile(effects_a, 25)  # Bottom 25% = most essential
    essential_a_mask = effects_a < threshold_a

    if np.sum(essential_a_mask) > 0:
        # Average effect of gene B when gene A is essential
        conditional_effect_b = np.mean(effects_b[essential_a_mask])
        features['depmap_conditional_dependency'] = conditional_effect_b
    else:
        features['depmap_conditional_dependency'] = 0

    # 4. MUTUAL ESSENTIALITY
    # How often are both genes essential in the same cell lines?
    threshold_b = np.percentile(effects_b, 25)
    essential_b_mask = effects_b < threshold_b

    mutual_essentiality = np.sum(essential_a_mask & essential_b_mask) / len(effects_a)
    features['depmap_mutual_essentiality'] = mutual_essentiality

    # 5. DIFFERENTIAL ESSENTIALITY
    # Standard deviation of the difference
    diff = effects_a - effects_b
    features['depmap_essentiality_diff_std'] = np.std(diff)
    features['depmap_essentiality_diff_mean'] = np.mean(np.abs(diff))

    # 6. INDIVIDUAL GENE STATISTICS
    features['depmap_mean_effect_a'] = np.mean(effects_a)
    features['depmap_mean_effect_b'] = np.mean(effects_b)
    features['depmap_std_effect_a'] = np.std(effects_a)
    features['depmap_std_effect_b'] = np.std(effects_b)

    # 7. ESSENTIALITY SCORES
    # Negative mean = essential gene
    features['depmap_is_essential_a'] = 1 if np.mean(effects_a) < -0.5 else 0
    features['depmap_is_essential_b'] = 1 if np.mean(effects_b) < -0.5 else 0

    # 8. COMPLEMENTARY ESSENTIALITY (key for SL)
    # One gene essential, other not
    features['depmap_complementary'] = abs(
        features['depmap_is_essential_a'] - features['depmap_is_essential_b']
    )

    return features

def extract_features_for_pair(gene_a, gene_b, genesdf):
    """
    Extract all features for a gene pair.

    Parameters:
    -----------
    gene_a, gene_b : str
        Gene symbols

    Returns:
    --------
    features : dict
    """
    features = {}
    features.update(compute_codependency_features(gene_a, gene_b, genesdf))
    features.update(compute_mutation_context_features(gene_a, gene_b, cell_line_mutations))
    return features

def generate_negative_pairs(n_pairs, known_sl_pairs, genesdf):
    """Generate random gene pairs as negative examples."""
    genes = list(genesdf.columns)
    negative_pairs = []

    # Normalize known SL pairs (handle both orderings)
    known_sl_set = set()
    for gene_a, gene_b in known_sl_pairs:
        # Add both orderings since (A,B) == (B,A)
        known_sl_set.add(tuple(sorted([gene_a, gene_b])))

    print(f"Excluding {len(known_sl_set)} known SL pairs from negatives...")

    max_attempts = n_pairs * 10  # Prevent infinite loop
    attempts = 0

    while len(negative_pairs) < n_pairs and attempts < max_attempts:
        gene_a, gene_b = np.random.choice(genes, size=2, replace=False)
        candidate_pair = tuple(sorted([gene_a, gene_b]))

        # Check if this pair is in known SL pairs
        if candidate_pair not in known_sl_set:
            negative_pairs.append((gene_a, gene_b))
        else:
            print(f"  Skipped known SL pair: {candidate_pair}")

        attempts += 1

    if len(negative_pairs) < n_pairs:
        print(f"Warning: Could only generate {len(negative_pairs)} negative pairs")

    return negative_pairs

def validate_negative_pairs(negative_pairs, known_sl_pairs):
    """
    Validate that negative pairs don't overlap with known SL pairs.

    Parameters:AAAS
    -----------
    negative_pairs : list of tuples
        Candidate negative pairs
    known_sl_pairs : list of tuples
        Known SL pairs to exclude

    Returns:
    --------
    validated_pairs : list of tuples
        Negative pairs with no overlap
    """
    known_sl_set = set()
    for gene_a, gene_b in known_sl_pairs:
        known_sl_set.add(tuple(sorted([gene_a, gene_b])))

    validated_pairs = []
    removed_count = 0

    for gene_a, gene_b in negative_pairs:
        candidate_pair = tuple(sorted([gene_a, gene_b]))
        if candidate_pair not in known_sl_set:
            validated_pairs.append((gene_a, gene_b))
        else:
            removed_count += 1
            print(f"  Removed known SL pair from negatives: {candidate_pair}")

    if removed_count > 0:
        print(f"  Total removed: {removed_count} pairs")

    return validated_pairs

def create_training_dataset(sl_pairs, genesdf, negative_pairs=None, comp=True):
    """
    Create training dataset from known SL pairs and negative examples.

    Parameters:
    -----------
    sl_pairs : list of tuples
        Known synthetic lethal pairs: [(gene_a, gene_b), ...]
    genesdf : dataframe
        Gene effect data
    negative_pairs : list of tuples, optional
        Known non-SL pairs. If None, randomly sample gene pairs.

    Returns:
    --------
    final_df : DataFrame
        Feature matrix combined with labels
    gene_pairs : list
        Corresponding gene pairs
    """
    all_features = []
    all_labels = []
    all_pairs = []

    # Positive examples
    print(f"Processing {len(sl_pairs)} known SL pairs...")
    for gene_a, gene_b in sl_pairs:
        features = extract_features_for_pair(gene_a, gene_b, genesdf)
        # Only append features if the computation was successful and returned non-empty
        if features and features['depmap_pearson_correlation'] != 0:
            # Check for default values indicating empty_features
            all_features.append(features)
            if comp:
                all_labels.append(1)
            all_pairs.append((gene_a, gene_b))

    # Negative examples
    if comp:
        print("Generating random negative pairs...")
        if negative_pairs is None:
            # Ensure non-empty_features are passed to generate_negative_pairs
            negative_pairs = generate_negative_pairs(len(all_pairs) * 2, known_sl_pairs, genesdf)

        print(f"Processing {len(negative_pairs)} negative pairs...")
        for gene_a, gene_b in negative_pairs:
            features = extract_features_for_pair(gene_a, gene_b, genesdf)
            # Only append features if the computation was successful and returned non-empty
            if features and features['depmap_pearson_correlation'] != 0:
                all_features.append(features)
                all_labels.append(0)
                all_pairs.append((gene_a, gene_b))

    if comp:
        X = pd.DataFrame(all_features)
        y = pd.DataFrame(all_labels, columns=["is_synthetic_lethal"])
        z = pd.DataFrame(all_pairs, columns=["gene_a", "gene_b"])
        final_df = pd.concat([z, X, y], axis=1)
        final_df.to_csv('sl_gene_pairs_w_genes.csv', index=False)
        # Save the combined DataFrame to a CSV file
        print(f"\nDataset created:")
        print(f"  Total pairs: {len(z)}")
        print("Combined features and labels saved to 'sl_gene_pairs.csv'")
        print(f"  Positive (SL): {np.sum(y.values)} ({np.mean(y.values)*100:.2f}%) ")
        print(f"  Features: {len(X.columns)}")
    else:
        X = pd.DataFrame(all_features)
        y = pd.DataFrame(all_pairs, columns=["gene_a", "gene_b"])
        final_df = pd.concat([y, X], axis=1)
        print(f"\nDone")
        print(f"  Total pairs: {len(y)}")
        print(f"  Features: {len(X.columns)}")


    return final_df, all_pairs

train_df, gene_pairs = create_training_dataset(known_sl_pairs, genesdf, non_sl_pairs)

train_df.head(10)

train_df.tail(10)

train_df.dropna(inplace=True)
train_df.head()

train_df.isna().value_counts()

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, roc_auc_score
from sklearn.model_selection import GridSearchCV, KFold

seed = 42

X = train_df.drop(['is_synthetic_lethal', 'gene_a', 'gene_b'], axis=1)
y = train_df['is_synthetic_lethal']

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=seed, stratify=y
)

scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

folds = KFold(n_splits=5, shuffle=True, random_state=seed)

for fold_idx, (train_index, val_index) in enumerate(folds.split(X_train, y_train)):
    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

    print(f"\n--- Fold {fold_idx + 1} ---")

    rf_model = RandomForestClassifier(n_estimators=100, random_state=seed, class_weight='balanced')
    rf_model.fit(X_train_fold, y_train_fold)

    y_pred_fold = rf_model.predict(X_val_fold)
    y_proba_fold = rf_model.predict_proba(X_val_fold)[:, 1]

    print(classification_report(y_val_fold, y_pred_fold, target_names=['Not SL', 'SL']))
    print(f"AUC-ROC (Fold {fold_idx + 1}): {roc_auc_score(y_val_fold, y_proba_fold):.4f}")

rf_model.feature_importances_

import seaborn as sns

# Get feature importances from the trained Random Forest model
feature_importances = rf_model.feature_importances_

# Get feature names from the original DataFrame before scaling
feature_names = X.columns

# Create a DataFrame for better visualization
feature_importance_df = pd.DataFrame({
    'Feature': feature_names,
    'Importance': feature_importances
})

# Sort the features by importance in descending order
feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)

# Plot the feature importances
plt.figure(figsize=(10, 6))
sns.barplot(x='Importance', y='Feature', data=feature_importance_df)
plt.title('Random Forest Feature Importances')
plt.xlabel('Importance')
plt.ylabel('Feature')
plt.tight_layout()
plt.show()

import xgboost as xgb

for fold_idx, (train_index, val_index) in enumerate(folds.split(X_train, y_train)):
    X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]
    y_train_fold, y_val_fold = y_train.iloc[train_index], y_train.iloc[val_index]

    print(f"\n--- Fold {fold_idx + 1} ---")

    # Initialize and train the XGBoost classifier
    xgb_model = xgb.XGBClassifier(
        objective='binary:logistic', # For binary classification
        eval_metric='logloss',       # Evaluation metric
        use_label_encoder=False,     # Suppress warning for deprecated parameter
        n_estimators=100,            # Number of boosting rounds
        random_state=seed,           # For reproducibility
        scale_pos_weight=(len(y_train_fold) - sum(y_train_fold)) / sum(y_train_fold) # Handle class imbalance
    )
    xgb_model.fit(X_train_fold, y_train_fold)

    # Make predictions and evaluate the XGBoost model
    y_pred_xgb = xgb_model.predict(X_val_fold)
    y_proba_xgb = xgb_model.predict_proba(X_val_fold)[:, 1]

    print("\n=== XGBoost Model Performance ===")
    print(classification_report(y_val_fold, y_pred_xgb, target_names=['Not SL', 'SL']))
    print(f"AUC-ROC (XGBoost): {roc_auc_score(y_val_fold, y_proba_xgb):.4f}")

test_gene_sl.head()

comp_sl_pairs = []

for gene_a, gene_b in zip(test_gene_sl.x_name, test_gene_sl.y_name):
    # Ensure both gene names are strings and not NaN before adding to the list
    if isinstance(gene_a, str) and isinstance(gene_b, str):
        comp_sl_pairs.append((gene_a, gene_b))

print(comp_sl_pairs)

test_df, test_pairs = create_training_dataset(comp_sl_pairs, genesdf, comp=False)

test_df.head()

test_df.dropna(inplace=True)
test_df.head()

X_test = test_df.drop(['gene_a', 'gene_b'], axis=1)

X_test = scaler.transform(X_test)

test_pred_xgb = xgb_model.predict(X_test)
test_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]

test_proba_xgb

import joblib

# Save the xgb_model
joblib.dump(xgb_model, 'xgb_model.joblib')
print("XGBoost model saved as 'xgb_model.joblib'")

# threshold

for i in range(len(test_proba_xgb)):
    if test_pred_xgb[i] == 0 and test_proba_xgb[i] < 0.5:
        test_pred_xgb[i] = 1

test_pred_xgb

pred = pd.DataFrame(test_pred_xgb, columns=['is_synthetic_lethal'])

pred['is_synthetic_lethal'].value_counts()

